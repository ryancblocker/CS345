{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A7 Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By typing my name, I confirm that the code, experiments, results, and discussions are all written by me, except for the code provided by the instructor.  \n",
    "\n",
    "*Ryan Blocker*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T16:14:18.933742Z",
     "start_time": "2023-04-14T16:14:18.194376Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\">**70 points**</font>: Code Requirements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code cell, define the function\n",
    "\n",
    "    def run_maze_experiment(n_steps, learning_rate, steps_between_goal_changes=0):\n",
    "        ...\n",
    "    \n",
    "that combines all of the code from Lecture Notes [21 More Reinforcement Learning Fun](https://nbviewer.org/url/www.cs.colostate.edu/~anderson/cs345/notebooks/21%20More%20Reinforcement%20Learning%20Fun.ipynb).  You can simply copy the functions in Lecture Notes 21 and paste them into the body of your `run_maze_experiment` function.  No global variables are allowed; all variables and functions defined globally in Lecture Notes 21 must be defined locally inside `run_maze_experiment`, including the goal of `[6, 6]`.  Assign a default value of `0` for the third argument, `steps_between_goal_changes`. \n",
    "\n",
    "Test your function by running it as\n",
    "\n",
    "    run_maze_experiment(100_000, 0.2)\n",
    "    \n",
    "You should see results very similar to the results shown in Lecture Notes 21. \n",
    "\n",
    "Do not include these results in your submitted notebook.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T16:18:52.674905Z",
     "start_time": "2023-04-14T16:18:52.664187Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_maze_experiment(n_steps, learning_rate, steps_between_goal_changes=0):\n",
    "    \n",
    "    figure = plt.figure(figsize=(10, 12)) #figure size\n",
    "    size = 16 #size of maze\n",
    "    \n",
    "    n = size - 1\n",
    "    walls = [[0, (0, n)],  # bottom wall\n",
    "        [n, (0, n)],       # top wall\n",
    "        [(0, n), 0],       # left wall\n",
    "        [(0, n), n],       # right wall\n",
    "\n",
    "        [(3, 9), 9],       # box right wall\n",
    "        [(3, 9), 2],       # box bottom wall\n",
    "        [9, (4, 8)],       # box top wall\n",
    "        [3, (3, 8)],       # box bottom wall\n",
    "        [(4, 12), 12]]     # additional vertical wall\n",
    "\n",
    "    goal = np.array([6, 6]) #place goal inside the box\n",
    "    \n",
    "    actions = [(1, 0),  (-1, 0), (0, -1), (0, 1)] #directional movement actions\n",
    "    \n",
    "    def hit_walls(position, walls):\n",
    "        r, c = position    # r is position row, c is position column\n",
    "        for wall in walls:\n",
    "            if isinstance(wall[0], int):\n",
    "                # horizontal\n",
    "                row = wall[0]\n",
    "                cols = wall[1]\n",
    "                if r == row and cols[0] <= c <= cols[1]:\n",
    "                    return True\n",
    "            else:\n",
    "                # vertical wall\n",
    "                rows = wall[0]\n",
    "                col = wall[1]\n",
    "                if c == col and rows[0] <= r <= rows[1]:\n",
    "                    return True\n",
    "        return False\n",
    "\n",
    "    print(hit_walls((1, 1), walls))\n",
    "    for p in [(1, 1), (5, 5), (0, 0), (9, 5), (8, 5)]:\n",
    "        print('Position', p, 'hit_walls=', hit_walls(p, walls))\n",
    "        \n",
    "    def take_action(position, actioni):\n",
    "        action = actions[actioni]\n",
    "        next_position = [position[0] + action[0],\n",
    "                        position[1] + action[1]]\n",
    "        return np.clip(next_position, 0, size - 1)\n",
    "\n",
    "    [3, 2]\n",
    "    for actioni in range(4):\n",
    "        print('Position', p, end='')\n",
    "        print(' action', actioni, actions[actioni], end='')\n",
    "        p = take_action(p, actioni)\n",
    "        print(' takes us to position', p)\n",
    "                \n",
    "    def pick_random_position(walls):\n",
    "        while True:\n",
    "            position = np.random.randint(1, size - 2, 2)\n",
    "            if not hit_walls(position, walls):\n",
    "                break\n",
    "        return position\n",
    "\n",
    "    for i in range(10):\n",
    "        print(pick_random_position(walls))\n",
    "        \n",
    "    Q = np.zeros((size, size, 4))\n",
    "    \n",
    "    def pick_action(Q, position):\n",
    "        row, col = position\n",
    "        Qs = Q[row, col, :]\n",
    "        return np.argmin(Qs)\n",
    "\n",
    "    actioni = pick_action(Q, [1, 1])\n",
    "    actioni, actions[actioni]\n",
    "    \n",
    "    position = pick_random_position(walls)\n",
    "    actioni = pick_action(Q, position)\n",
    "\n",
    "    n_goals = 0\n",
    "    steps_to_goal = []\n",
    "    last_path = []\n",
    "    starting_step = 0\n",
    "    goal_found = False\n",
    "\n",
    "    for step in range(n_steps):\n",
    "\n",
    "        #CODE for multiple goals\n",
    "        if step % steps_between_goal_changes == 0:\n",
    "            goal = pick_random_position()\n",
    "            position = pick_random_position()\n",
    "            actioni = pick_action(Q, position, goal)\n",
    "            last_path = [position]\n",
    "            \n",
    "        if goal_found:\n",
    "            last_path = []\n",
    "            goal_found = False\n",
    "        \n",
    "        next_position = take_action(position, actioni)\n",
    "        last_path.append(next_position)\n",
    "        \n",
    "        if hit_walls(next_position, walls):\n",
    "            row, col = position\n",
    "            Q[row, col, actioni] = 500  # Make Q so high this action never selected again.\n",
    "            last_path.append(position)\n",
    "            actioni = pick_action(Q, position)\n",
    "            \n",
    "        elif np.all(next_position == goal):\n",
    "            # Found goal\n",
    "            goal_found = True\n",
    "            n_goals += 1\n",
    "            r = 1\n",
    "            row, col = position\n",
    "            Q[row, col, actioni] = r  # No future. Just found the goal.\n",
    "            # Start at new random position\n",
    "            position = pick_random_position(walls)\n",
    "            actioni = pick_action(Q, position)\n",
    "            steps_to_goal.append(step - starting_step)\n",
    "            starting_step = step\n",
    "\n",
    "        else:\n",
    "            # Take one step to get next_Q at next position to make TD error\n",
    "            r = 1\n",
    "            next_actioni = pick_action(Q, next_position) \n",
    "            Q_value = Q[position[0], position[1], actioni]\n",
    "            next_Q_value = Q[next_position[0], next_position[1], next_actioni]\n",
    "            TD_error = r + next_Q_value - Q_value\n",
    "            Q[position[0], position[1], actioni] += learning_rate * TD_error\n",
    "\n",
    "            position = next_position.copy()\n",
    "            actioni = next_actioni\n",
    "            \n",
    "\n",
    "        if goal_found and (n_goals < 100 or n_goals % 100 == 0):\n",
    "\n",
    "            figure.clf()\n",
    "            \n",
    "            # Draw Q function as image with walls\n",
    "            image = np.min(Q[:, :, :], axis=-1)\n",
    "            imagemax = np.max(image)\n",
    "            vmax = imagemax if imagemax > 5 else 5\n",
    "            plt.figure(1)\n",
    "            plt.clf()\n",
    "            plt.subplot(2, 1, 1)\n",
    "            plt.imshow(image, origin='lower', cmap='binary',\n",
    "                    interpolation='nearest')\n",
    "            plt.colorbar()\n",
    "            \n",
    "            # Draw walls\n",
    "            for row in range(size):\n",
    "                for col in range(size):\n",
    "                    if hit_walls([row, col], walls):\n",
    "                        plt.plot(col, row, 'rs', ms=10)\n",
    "\n",
    "            # Draw last path\n",
    "            last_path_array = np.array(last_path)\n",
    "            plt.plot(last_path_array[:, 1], last_path_array[:, 0], 'o-')\n",
    "\n",
    "            # Draw goal\n",
    "            plt.plot(goal[1], goal[0], 'mD', ms=10)\n",
    "\n",
    "            # Plot steps to goal for each path tried\n",
    "            plt.subplot(2, 1, 2)\n",
    "            plt.plot(steps_to_goal)\n",
    "            plt.xlabel('Goals Found')\n",
    "            plt.ylabel('Steps to Goal')\n",
    "            \n",
    "            clear_output(wait=True)\n",
    "            display(figure);\n",
    "\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    return Q  # Return Q table!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you see that your code runs correctly, make changes to your `run_maze_experiment` to change the goal while training.  Required changes will include at least the following steps:\n",
    "\n",
    "* Change the dimensionality of the `Q` table to include dimensions for the goal's row and column.\n",
    "* The first code immediately following the start of the `step` loop must look like this:\n",
    "\n",
    "```\n",
    "    for step in range(n_steps):\n",
    "\n",
    "        # NEW CODE for multiple goals\n",
    "        if steps_between_goal_changes > 0 and step % steps_between_goal_changes == 0:\n",
    "            goal = pick_random_position()\n",
    "            position = pick_random_position()\n",
    "            actioni = pick_action(Q, position, goal)\n",
    "            last_path = [position]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\">**20 points**</font>: Show Results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After debugging your function test that it still runs correctly with `steps_between_goal_changes=0`, which should produce results similar to what is shown here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T16:25:56.593693Z",
     "start_time": "2023-04-14T16:24:04.979529Z"
    }
   },
   "outputs": [],
   "source": [
    "Q = run_maze_experiment(100_000, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run it with the following arguments to test your changes.  You should see displays like the ones above, but for varying goal positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T16:35:43.579552Z",
     "start_time": "2023-04-14T16:33:20.994669Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Q = run_maze_experiment(1_000_000, 0.2, steps_between_goal_changes=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\">**20 points**</font>: Visualize Result of Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your function returns the trained `Q` table.  Let's investigate what has been learned. A well-trained `Q` table should have values close to zero near the goal, whereever it is placed!  So, let's display images of the trained `Q` table for different goals.\n",
    "\n",
    "Complete the following code cell.  A list of nine goals is provided, where each goal is specified by its row and column. Make a figure of three columns and three rows of images (using `plt.subplot`, `plt.imshow`, and `plt.colorbar` as shown in Lecture Notes 21) of the `Q` table values for these nine goal positions.\n",
    "\n",
    "Do not include the walls in the displays, but do plot the goal position with a magenta diamond."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T16:36:09.608601Z",
     "start_time": "2023-04-14T16:36:07.356381Z"
    }
   },
   "outputs": [],
   "source": [
    "goals = [[12, 3], [12, 7], [12, 13],\n",
    "         [7, 3], [7, 7], [7, 13],\n",
    "         [1, 3], [1, 7], [1, 13]]\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "\n",
    "# ....\n",
    "# ....\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\">**10 points**</font>: Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discuss these questions in the following markdown cell.\n",
    "\n",
    "What do your images of the `Q` table values show?  Do you think your reinforcement learning agent has learned to solve this maze problem with varying goal?\n",
    "\n",
    "Why are the walls shown as white patches?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...type your answers here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T16:38:18.626736Z",
     "start_time": "2023-04-14T16:38:18.624246Z"
    }
   },
   "source": [
    "#  Extra Credit 1\n",
    "\n",
    "Design a larger maze of size 32 or larger.  Create different walls that define an \"interesting\" maze for our reinforcement learning bug.  Do this in a new version of your function named `run_big_maze_experiment` and run it with `steps_between_goal_changes` having a value greater than 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T16:38:18.626736Z",
     "start_time": "2023-04-14T16:38:18.624246Z"
    }
   },
   "source": [
    "#  Extra Credit 2\n",
    "\n",
    "Write code that uses your trained `Q` function to demonstrate your trained reinforcement learning bug following a goal that you place interactively by capturing `matplotlib` mouse click events on the display of the maze."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T16:38:18.626736Z",
     "start_time": "2023-04-14T16:38:18.624246Z"
    }
   },
   "source": [
    "#  Extra Credit 3\n",
    "\n",
    "This will take a considerable amount of effort.  Attempt this only if you are not already overwhelmed with requirements in other courses.\n",
    "\n",
    "Create a new version of your main function, called `run_neural_net_maze_experiment` as follows.\n",
    "\n",
    "Replace the `Q` table with your neural network from A6.  Give your neural network five inputs, being the agent's row and column, the goal's row and column, and the action. Your neural network will have just one output, the Q value for that position, goal, and action.  Train it with `X` being a sample of these five values, and a target `T` of `r + Qnew`.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
