{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\newcommand{\\xv}{\\mathbf{x}}\n",
    "\\newcommand{\\Xv}{\\mathbf{X}}\n",
    "\\newcommand{\\yv}{\\mathbf{y}}\n",
    "\\newcommand{\\Yv}{\\mathbf{Y}}\n",
    "\\newcommand{\\zv}{\\mathbf{z}}\n",
    "\\newcommand{\\av}{\\mathbf{a}}\n",
    "\\newcommand{\\Wv}{\\mathbf{W}}\n",
    "\\newcommand{\\wv}{\\mathbf{w}}\n",
    "\\newcommand{\\betav}{\\mathbf{\\beta}}\n",
    "\\newcommand{\\gv}{\\mathbf{g}}\n",
    "\\newcommand{\\Hv}{\\mathbf{H}}\n",
    "\\newcommand{\\dv}{\\mathbf{d}}\n",
    "\\newcommand{\\Vv}{\\mathbf{V}}\n",
    "\\newcommand{\\vv}{\\mathbf{v}}\n",
    "\\newcommand{\\Uv}{\\mathbf{U}}\n",
    "\\newcommand{\\uv}{\\mathbf{u}}\n",
    "\\newcommand{\\tv}{\\mathbf{t}}\n",
    "\\newcommand{\\Tv}{\\mathbf{T}}\n",
    "\\newcommand{\\Sv}{\\mathbf{S}}\n",
    "\\newcommand{\\Gv}{\\mathbf{G}}\n",
    "\\newcommand{\\zv}{\\mathbf{z}}\n",
    "\\newcommand{\\Zv}{\\mathbf{Z}}\n",
    "\\newcommand{\\Norm}{\\mathcal{N}}\n",
    "\\newcommand{\\muv}{\\boldsymbol{\\mu}}\n",
    "\\newcommand{\\sigmav}{\\boldsymbol{\\sigma}}\n",
    "\\newcommand{\\phiv}{\\boldsymbol{\\phi}}\n",
    "\\newcommand{\\Phiv}{\\boldsymbol{\\Phi}}\n",
    "\\newcommand{\\Sigmav}{\\boldsymbol{\\Sigma}}\n",
    "\\newcommand{\\Lambdav}{\\boldsymbol{\\Lambda}}\n",
    "\\newcommand{\\half}{\\frac{1}{2}}\n",
    "\\newcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n",
    "\\newcommand{\\argmin}[1]{\\underset{#1}{\\operatorname{argmin}}}\n",
    "\\newcommand{\\dimensionbar}[1]{\\underset{#1}{\\operatorname{|}}}\n",
    "\\newcommand{\\grad}{\\mathbf{\\nabla}}\n",
    "\\newcommand{\\ebx}[1]{e^{\\betav_{#1}^T \\xv_n}}\n",
    "\\newcommand{\\eby}[1]{e^{y_{n,#1}}}\n",
    "\\newcommand{\\Tiv}{\\mathbf{Ti}}\n",
    "\\newcommand{\\Fv}{\\mathbf{F}}\n",
    "\\newcommand{\\ones}[1]{\\mathbf{1}_{#1}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine a position in a tic-tac-toe game (knots and crosses).\n",
    "How do you choose the best  next action?\n",
    "\n",
    "<img src=\"http://www.cs.colostate.edu/~anderson/cs345/notebooks/figures/ttt1.png\" width=600>\n",
    "\n",
    "Which are you most likely to win from?\n",
    "Guess at how likely you are  to win from each state.  Is a win \n",
    "definite, likely, or maybe?\n",
    "\n",
    "<img src=\"http://www.cs.colostate.edu/~anderson/cs345/notebooks/figures/ttt2.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## States and Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set of possible states, $\\mathcal{S}$.\n",
    "\n",
    "   * Can be discrete values ($|\\mathcal{S}| < \\infty$)\n",
    "       * Tic-Tac-Toe game positions\n",
    "       * Position in a maze\n",
    "       * Sequence of steps in a plan\n",
    "   *  Can be continuous values ($|\\mathcal{S}| = \\infty$)\n",
    "       * Joint angles of a robot arm\n",
    "       * Position and velocity of a race car\n",
    "       * Parameter values for a network routing strategy \n",
    "\n",
    "Set of possible actions, $\\mathcal{A}$.\n",
    "\n",
    "   * Can be discrete values ($|\\mathcal{A}| < \\infty$)\n",
    "       *  Next moves in Tic-Tac-Toe \n",
    "       * Directions to step in a maze\n",
    "       * Rearrangements of a sequence of steps in a plan\n",
    "   * Can be continuous values ($|\\mathcal{A}| = \\infty$)\n",
    "       * Torques to apply to the joints of a robot arm\n",
    "       *  Fuel rate and turning torque in a race car\n",
    "       * Settings of parameter values for a network routing strategy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to choose the action that we predict will result in the best\n",
    "possible future from the current state.  Need a value that\n",
    "represents the future outcome.\n",
    "\n",
    "What should the value represent?\n",
    "\n",
    "   * Tic-Tac-Toe: Likelihood of winning from a game position.\n",
    "   * Maze: Number of steps to reach the goal.\n",
    "   * Plan: Efficiency in time and cost of accomplishing the objective  for particular rearrangment of steps in a plan.\n",
    "   * Robot: Energy required to move the gripper on a robot arm to a destination.\n",
    "   * Race car: Time to reach the finish line.\n",
    "   * Network routing: Throughput.\n",
    "\n",
    "With the correct values, multi-step decision problems are reduced\n",
    "to single-step decision problems.  Just pick action with best\n",
    "value.  Guaranteed to find optimal multi-step solution (dynamic programming).\n",
    "\n",
    "The utility or cost of a single action taken from a state is the **reinforcement**\n",
    "for that action from that state. The value of that state-action is\n",
    "the expected value of the full **return** or the sum of reinforcements that will follow\n",
    "when that action is taken.\n",
    "\n",
    "<img src=\"http://www.cs.colostate.edu/~anderson/cs345/notebooks/figures/returns1.png\">\n",
    "\n",
    "Say we are in state $s_t$ at time $t$.  Upon taking action $a_t$\n",
    "from that state we observe the one step reinforcement $r_{t+1}$,\n",
    "and the next state $s_{t+1}$. \n",
    "\n",
    "Say this continues until we reach a goal state, $K$ steps later.\n",
    "What is the return $R_t$ from state $s_t$?\n",
    "\n",
    "$$\n",
    "    \\begin{align*}\n",
    "      R_t = \\sum_{k=0}^K r_{t+k+1}\n",
    "    \\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "Use the returns to choose the best action.\n",
    "\n",
    "<img src=\"http://www.cs.colostate.edu/~anderson/cs345/notebooks/figures/returns2.png\">\n",
    "\n",
    "Right...are we maximizing or minimizing?  What does the\n",
    "reinforcement represent?  Let's say it is energy used that we want\n",
    "to minimize.  $a_1$, $a_2$, or $a_3$?\n",
    "\n",
    "Where do the values come from?\n",
    "\n",
    "   * Write the code to calculate them.    \n",
    "      * Usually not possible. If you can do this for your problem, why are you considering machine learning? Might be able to do this for Tic-Tac-Toe.\n",
    "   * Use dynamic programming.\n",
    "      *   Usually not possible. Requires knowledge of the probabilities of transitions between all states for all actions. \n",
    "   * Learn from examples, lots of examples. Lots of 5-tuples: state, action, reinforcement, next state, next action ($s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1}$).\n",
    "      *  **Monte Carlo:** Assign to each state-action pair an average of the observed returns: $ \\;\\;\\;\\text{value}(s_t,a_t) \\approx \\text{mean of } R(s_t,a_t)$\n",
    "      *  **Temporal Difference (TD):** Using $\\text{value}(s_{t+1},a_{t+1})$ as estimate of return from next state, update current state-action value: $\\;\\;\\; \\text{value}(s_t,a_t) \\approx r_{t+1} + \\text{value}(s_{t+1},a_{t+1})$\n",
    "\n",
    "What is the estimate of the return $R$ from state B?\n",
    "\n",
    "<img src=\"http://www.cs.colostate.edu/~anderson/cs345/notebooks/figures/tdadvantage.png\">\n",
    "\n",
    "   * **Monte Carlo:** $\\text{mean of } R(s_t,a_t)$ =  1, a prediction of a win\n",
    "   * **Temporal Difference (TD):** $r_{t+1} +  \\text{value}(s_{t+1},a_{t+1}) =  0 + (100(-1) + 2(1))/100 =  -0.98$, a very likely loss\n",
    "   * What do you do? The green pill  or the red pill?\n",
    "   * TD takes advantage of the cached experience given in the value learned for State C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Maze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here is a simple maze.\n",
    "\n",
    "<img src=\"http://www.cs.colostate.edu/~anderson/cs345/notebooks/figures/mazepic.png\">\n",
    "\n",
    "From any position, how do you decide whether to move up, right, down, or left?\n",
    "\n",
    "Right.  Need an estimate of the number of steps to reach the\n",
    "goal. This will be the return $R$. How do we  formulate this in terms of\n",
    "reinforcements?\n",
    "\n",
    "Yep. $r_t = 1$ for every step.  Then $R_t = \\sum_{k=0}^K r_{t+k+1}$ will sum of those 1's to produce the number of steps to\n",
    "goal from each state.\n",
    "\n",
    "The Monte-carlo way will assign value as average of number of steps to goal from each\n",
    "starting state tried.\n",
    "\n",
    "The TD way will update value based on (1 + estimated value from next state).\n",
    "\n",
    "\n",
    "Should we do Monte-Carlo update or Temporal-Difference updates?  Take a look at this comparison on the maze problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State-Action Value Function as a Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabular Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the state-action value function is a function of\n",
    "both state and action and its value is a prediction of the\n",
    "expected sum of future reinforcements.\n",
    "\n",
    "We will call the state-action value function $Q$, after\n",
    "[Learning from Delayed Rewards](http://www.cs.rhul.ac.uk/~chrisw/thesis.html), by C. Watkins, PhD\n",
    "Thesis, University of Cambridge, Cambridge, UK, 1989.\n",
    "\n",
    "We can select our current belief of what the optimal action, $a_t^o$, is in state $s_t$ by\n",
    "\n",
    "$$\n",
    "    \\begin{align*}\n",
    "      a_t^o = \\argmax{a} Q(s_t,a)\n",
    "    \\end{align*}\n",
    "$$\n",
    "\n",
    "or\n",
    "\n",
    "$$\n",
    "    \\begin{align*}\n",
    "      a_t^o = \\argmin{a} Q(s_t,a)\n",
    "    \\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabular Q for the Maze Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representing the Q Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the maze world,\n",
    "\n",
    "$$\n",
    "    \\begin{align*}\n",
    "      a_t^o = \\argmin{a} Q(s_t,a)\n",
    "    \\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "looks like (argmax should be argmin)\n",
    "\n",
    "<img src=\"http://www.cs.colostate.edu/~anderson/cs345/notebooks/figures/qactionmaze-crop.png\">\n",
    "\n",
    "A bit more mathematically, let the current state be given by\n",
    "position in $x$ and $y$ coordinates and actions are integers 1 to\n",
    "4. Then\n",
    "\n",
    "$$\n",
    "    \\begin{align*}\n",
    "      a_t^o = \\argmin{a\\in \\{1,2,3,4\\}} Q\\left ((x,y), a\\right )\n",
    "    \\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "Now, let's try to do this in python.\n",
    "\n",
    "How do we implement the Q function?  For the maze problem, we know we can\n",
    "\n",
    "   *  enumerate all the states (positions) the set of which is finite ($10\\cdot 10$),\n",
    "   *  enumerate all actions, the set of which is finite (4),\n",
    "   *  calculate the new state from the old state and an action, and\n",
    "   *  represent in memory all state-action combinations ($10\\cdot 10\\cdot 4$).\n",
    "\n",
    "So, let's just store the Q function in table form.\n",
    "\n",
    "The Q table will need three dimensions, for $x$, $y$, and the action.\n",
    "\n",
    "<img src=\"http://www.cs.colostate.edu/~anderson/cs345/notebooks/figures/qtableMaze.png\">\n",
    "\n",
    "How do we look up the Q values for a state?\n",
    "\n",
    "<img src=\"http://www.cs.colostate.edu/~anderson/cs345/notebooks/figures/qtableMazeState.png\">\n",
    "\n",
    "Q values are steps to goal, so we are minimizing.  Select right or down action.\n",
    "\n",
    "We are finally ready for python.  How can we make a three-dimensional table of Q values, if $x$ and $y$ have 10 possible values\n",
    "and we have 4 actions?\n",
    "\n",
    "     import numpy as np\n",
    "     Q = np.zeros((10, 10, 4))\n",
    "\n",
    "How should we initialize the table?  Above line initializes all values to be zero.  What effect will\n",
    "this have as Q values for actions taken are updated to estimate steps to goal?\n",
    "\n",
    "Actions not yet tried will have lowest (0) Q value. Forces\n",
    "the agent to try all actions from all states---lots of **exploration**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating the Q Table Using Temporal-Difference Updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What must we do after observing $s_t$, $a_t$, $r_{t+1}$, $s_{t+1}$, and $a_{t+1}$?\n",
    "\n",
    "Calculate the temporal-difference error $r_{t+1} + Q(s_{t+1},a_{t+1}) - Q(s_t,a_t)$ and use it to\n",
    "update the Q value stored for $s_t$ and $a_t$:\n",
    "\n",
    "$$\n",
    "    \\begin{align*}\n",
    "      Q(s_t,a_t) = Q(s_t,a_t) + \\rho (r_{t+1} + Q(s_{t+1},a_{t+1}) - Q(s_t,a_t))\n",
    "    \\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "And, in python? Assume position, or state, $(2, 3)$ is implemented as ''state = np.array([2, 3])''.\n",
    "\n",
    "    r = 1\n",
    "    Qold = Q[stateOld[0], stateOld[1], actionOld]\n",
    "    Qnew = Q[state[0], state[1], action]\n",
    "    TDError = r + Qnew - Qold\n",
    "    Q[stateOld[0], stateOld[1], actionOld] = Qold + rho * TDError\n",
    "\n",
    "\n",
    "This is performed for every pair of steps $t$ and $t+1$, until the final step,\n",
    "which must be handled differently.  There is no $s_{t+1}$.\n",
    "The update\n",
    "\n",
    "$$\n",
    "    \\begin{align*}\n",
    "      Q(s_t,a_t) = Q(s_t,a_t) + \\rho (r_{t+1} + Q(s_{t+1},a_{t+1}) - Q(s_t,a_t))\n",
    "    \\end{align*}\n",
    "$$\n",
    "\n",
    "becomes\n",
    "\n",
    "$$\n",
    "    \\begin{align*}\n",
    "      Q(s_t,a_t) = Q(s_t,a_t) + \\rho (r_{t+1}  - Q(s_t,a_t))\n",
    "    \\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "In python, add a test for being at the goal. Let ''maze'' be character array containing a ''G'' at the\n",
    "goal position.\n",
    "\n",
    "    r = 1\n",
    "    Qold = Q[stateOld[0], stateOld[1], actionOld]\n",
    "    Qnew = Q[state[0], state[1], action]\n",
    "\n",
    "    if (maze[state[0]+1,state[1]+1] == 'G'):\n",
    "        TDerror = r - Qold\n",
    "    else:\n",
    "        TDerror = r + Qnew - Qold\n",
    "\n",
    "    Q[stateOld[0], stateOld[1], actionOld] = Qold + rho * TDerror\n",
    "\n",
    "\n",
    "To choose the best action for state $(x, y)$ stored in variable state,\n",
    "just need to do\n",
    "\n",
    "     a = np.argmin(Q[state[0], state[1], :])\n",
    "\n",
    "and if we store the available actions as\n",
    "\n",
    "     actions = np.array([[0, 1], [1, 0], [0, -1], [-1, 0]])\n",
    "\n",
    "then the update to state based on action a is done by\n",
    "\n",
    "     state = state + actions[a, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Agent-Environment Interaction Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our agent to interact with its world, we must implement the steps\n",
    "\n",
    "   1.  Initialize Q.\n",
    "   1.  Choose random, non-goal, state.\n",
    "   1. Repeat:\n",
    "       1. If at goal,\n",
    "        1. Update Qold with TD error (1 - Qold)\n",
    "        1. Pick new random state\n",
    "       1. Otherwise (not at goal),\n",
    "         1. Select next action.\n",
    "         1. If not first step, update Qold with TD error (1 + Qnew - Qold)\n",
    "         1.  Shift current state and action to old ones.\n",
    "         1. Apply action to get new state.\n",
    "\n",
    "In Python it would look something like the following for a 10x10 maze.\n",
    "\n",
    "     Q = np.zeros((10,10,4))                  # 1.\n",
    "     s = np.random.randint(0,10,2)            # 2.\n",
    "     for step in xrange(10000):               # 3. (or forever)\n",
    "         if isGoal(s):                        # 3.A.\n",
    "             Q[sOld[0],sOld[1],aOld] +=       # 3.A.a\n",
    "                  rho * (1 - Q[sOld[0],sOld[1],aOld])\n",
    "             s = np.random.randint(0,10,2)    # 3.A.b\n",
    "         else:                                # 3.B\n",
    "             a = np.argmin(Q[s[0],s[1],:])    # 3.B.a\n",
    "             if steps > 1:\n",
    "                 Q[sOld[0],sOld[1],aOld] +=   # 3.B.b\n",
    "                      rho * (1 + Q[s[0],s[1],a] - Q[sOld[0],sOld[1],aOld])\n",
    "             sOld, aOld = s, a                # 3.B.c\n",
    "             s += actions[a,:]                # 3.B.d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Solution of the Maze Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we start with a text file that specifies a maze.  Let's use the cell magic %%writefile to make the maze file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-06T15:36:46.722962Z",
     "start_time": "2023-04-06T15:36:46.140999Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-06T15:36:49.439570Z",
     "start_time": "2023-04-06T15:36:49.433994Z"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile maze1.txt\n",
    "************\n",
    "*          *\n",
    "*          *\n",
    "*        * *\n",
    "*        * *\n",
    "*        * *\n",
    "*     **** *\n",
    "*     * G* *\n",
    "*     *  * *\n",
    "*     *  * *\n",
    "*          *\n",
    "************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-06T15:36:58.198549Z",
     "start_time": "2023-04-06T15:36:58.082774Z"
    }
   },
   "outputs": [],
   "source": [
    "!cat maze1.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-06T15:37:14.515506Z",
     "start_time": "2023-04-06T15:37:14.511736Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('maze1.txt') as f:\n",
    "    for line in f:\n",
    "        print(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-06T15:37:16.156274Z",
     "start_time": "2023-04-06T15:37:16.150095Z"
    }
   },
   "outputs": [],
   "source": [
    "mazelist = []\n",
    "with open('maze1.txt') as f:\n",
    "    for line in f:\n",
    "        mazelist.append(line.strip())\n",
    "mazelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-06T15:37:16.995762Z",
     "start_time": "2023-04-06T15:37:16.990948Z"
    }
   },
   "outputs": [],
   "source": [
    "maze = np.array(mazelist).view('U1').reshape((len(mazelist), len(mazelist[0])))\n",
    "print(maze.shape)\n",
    "maze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-06T15:37:18.323030Z",
     "start_time": "2023-04-06T15:37:18.319167Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(maze.shape[0]):\n",
    "    for j in range(maze.shape[1]):\n",
    "        print(maze[i,j],end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need some functions, one to draw the Q surface, over the two-dimensional state space (position in the maze), and one to select an action given the Q surface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-06T15:37:43.412152Z",
     "start_time": "2023-04-06T15:37:43.405290Z"
    }
   },
   "outputs": [],
   "source": [
    "### Draw Q surface, showing minimum Q value for each state\n",
    "\n",
    "def showQ(Q, title, ax):\n",
    "    (m, n, _) = Q.shape\n",
    "    gridsize = max(m, n)\n",
    "    rows = np.floor(np.linspace(0, m - 0.5, gridsize))\n",
    "    cols = np.floor(np.linspace(0, n - 0.5, gridsize))\n",
    "    ygrid, xgrid = np.meshgrid(rows, cols, indexing='ij')\n",
    "    points = np.vstack((ygrid.flat, xgrid.flat))\n",
    "    Qmins = [np.min( Q[int(s1), int(s2), :]) for (s1, s2) in zip(points[0, :], points[1, :])]\n",
    "    Qmins = np.asarray(Qmins).reshape(xgrid.shape)\n",
    "    ax.plot_surface(xgrid, ygrid, Qmins, color='yellow')\n",
    "    plt.ylim(m - 1 + 0.5, 0 - 0.5)\n",
    "    ax.set_zlabel('Qmin')\n",
    "    ax.set_title(f'Min {np.min(Qmins):.1f} Max {np.max(Qmins):.1f}')\n",
    "\n",
    "### Show current policy\n",
    "\n",
    "def showPolicy(Q):\n",
    "    (m, n, _) = Q.shape\n",
    "    bestactions = np.argmin(Q, axis=2)\n",
    "    prow, pcol = np.meshgrid(np.arange(m), np.arange(n), indexing='ij')\n",
    "    arrowrow = actions[:, 0][bestactions]\n",
    "    arrowcol = actions[:, 1][bestactions]\n",
    "    plt.quiver(pcol, prow, arrowcol, -arrowrow)\n",
    "    walls_row, walls_col = np.where(maze[1:-1, 1:-1] == '*') \n",
    "    plt.plot(walls_col, walls_row, 'ro', ms=15, alpha=0.5)\n",
    "    goal_row, goal_col = np.where(maze[1:-1, 1:-1] == 'G')\n",
    "    plt.plot(goal_col, goal_row, 'go', ms=15, alpha=0.5)\n",
    "    plt.ylim(m - 1 + 0.5, 0 - 0.5)\n",
    "    plt.xlim(0 - 0.5, n - 1 + 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct arrays to hold the tabular Q values updated by temporal differences, and one to hold Q values updated by Monte Carlo.  Set Q values to *np.inf* for invalid actions.  We have four possible actions from each position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-06T15:41:02.224368Z",
     "start_time": "2023-04-06T15:41:02.219497Z"
    }
   },
   "outputs": [],
   "source": [
    "def init_Q(maze):\n",
    "    m, n = maze.shape\n",
    "    m -= 2  # for ceiling and floor\n",
    "    n -= 2  # for walls\n",
    "    Q = np.zeros((m, n, 4))\n",
    "    Qmc = np.zeros((m, n, 4))\n",
    "    actions = np.array([[0, 1], [1, 0], [0, -1], [-1, 0]])  # changes in row and column position of RL agent\n",
    "\n",
    "    ### Set Q value of invalid actions to np.inf\n",
    "    for mi in range(m):\n",
    "        for ni in range(n):\n",
    "            for ai in range(4):\n",
    "                r = mi + actions[ai, 0]\n",
    "                c = ni + actions[ai, 1]\n",
    "                if maze[r + 1, c + 1] == '*':  # showing ai was invalid action\n",
    "                    Q[mi, ni, ai] = np.inf\n",
    "                    Qmc[mi, ni, ai] = np.inf\n",
    "                    \n",
    "    return Q, Qmc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for some parameters.  Let's run for 100,000 interactions with maze environment, so 100,000 updates, and let $\\rho$, the learning rate, be 0.1 and $\\epsilon$, the random action probability, be 0.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-06T15:39:31.632029Z",
     "start_time": "2023-04-06T15:39:31.629767Z"
    }
   },
   "outputs": [],
   "source": [
    "nSteps = 100000\n",
    "rho = 0.1\n",
    "epsilon = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to keep a history, or trace, of positions and reinforcement, to be used to update the MC version of Q."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-06T15:38:01.961880Z",
     "start_time": "2023-04-06T15:38:01.959000Z"
    }
   },
   "outputs": [],
   "source": [
    "trace = np.zeros((nSteps, 3)) # for x, y, and a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-06T15:40:46.990205Z",
     "start_time": "2023-04-06T15:40:46.987240Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need some initializations before starting the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-06T15:42:19.119588Z",
     "start_time": "2023-04-06T15:41:42.580809Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "Q, Qmc = init_Q(maze)\n",
    "\n",
    "nSteps = 100000\n",
    "rho = 0.1\n",
    "epsilon = 0.2\n",
    "trace = np.zeros((nSteps, 3)) # for x, y, and a\n",
    "\n",
    "s = np.array([1, 1])  # start position\n",
    "a = 1 #first action (index)\n",
    "trials = []\n",
    "steps = 0\n",
    "goals = 0\n",
    "\n",
    "for step in range(nSteps):\n",
    "    trace[steps, :] = s.tolist() + [a]\n",
    "    here = maze[s[0] + 1, s[1] + 1]\n",
    "    if here == 'G':\n",
    "        # Found the Goal!\n",
    "        goals += 1\n",
    "        Q[s[0], s[1], a] = 0\n",
    "        if steps > 0:\n",
    "            Q[sold[0], sold[1], aold] += rho * (1 - Q[sold[0], sold[1], aold])\n",
    "            \n",
    "        # Monte Carlo update\n",
    "        cost = 0\n",
    "        for sai in range(steps, -1, -1):\n",
    "            r, c, act = trace[sai, :]\n",
    "            r, c, act = int(r), int(c), int(act)\n",
    "            Qmc[r, c, act] = (1 - rho) * Qmc[r, c, act] + rho * cost\n",
    "            cost += 1\n",
    "\n",
    "        s = np.array([np.random.randint(0, m), np.random.randint(0, n)])\n",
    "        #sold = []\n",
    "        trials.append(steps)\n",
    "    \n",
    "    else:\n",
    "        # Not goal\n",
    "        steps += 1    \n",
    "        Qfunc = Q  # Qfunc = Qmc # to use Monte Carlo policy to drive behavior\n",
    "    \n",
    "        # Pick next action a\n",
    "        if np.random.uniform() < epsilon:\n",
    "            validActions = [a for (i, a) in enumerate(range(4)) if not np.isinf(Qfunc[s[0], s[1], i])]\n",
    "            a = np.random.choice(validActions)\n",
    "        else:\n",
    "            a = np.argmin(Qfunc[s[0], s[1], :])\n",
    "\n",
    "        if steps > 1:\n",
    "            Q[sold[0], sold[1], aold] += rho * (1 + Q[s[0], s[1], a] - Q[sold[0], sold[1], aold])\n",
    "\n",
    "        sold = s\n",
    "        aold = a\n",
    "        s = s + actions[a, :]\n",
    "\n",
    "    if (here == 'G' and goals % 100 == 0):\n",
    "        \n",
    "        fig.clf()\n",
    "        \n",
    "        ax = fig.add_subplot(3, 2, 1, projection='3d')\n",
    "        showQ(Q, 'TD', ax)\n",
    "\n",
    "        ax = fig.add_subplot(3, 2, 2, projection='3d')\n",
    "        showQ(Qmc, 'Monte Carlo', ax)\n",
    "        plt.subplot(3, 2, 3)\n",
    "        showPolicy(Q)\n",
    "        plt.title('Q Policy')\n",
    "\n",
    "        plt.subplot(3, 2, 4)\n",
    "        showPolicy(Qmc)\n",
    "        plt.title('Monte Carlo Q Policy')\n",
    "\n",
    "        plt.subplot(3, 2, 5)\n",
    "        plt.plot(trace[:steps + 1, 1], trace[:steps + 1, 0], 'o-')\n",
    "        plt.plot(trace[0, 1], trace[0, 0], 'ro')\n",
    "        plt.xlim(0 - 0.5, 9 + 0.5)\n",
    "        plt.ylim(9 + 0.5, 0 - 0.5)\n",
    "        plt.title('Most Recent Trial')\n",
    "\n",
    "        plt.subplot(3, 2, 6)\n",
    "        plt.plot(trials, '-')\n",
    "        plt.xlabel('Trial')\n",
    "        plt.ylabel('Steps to Goal')\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        display(fig);\n",
    "\n",
    "    if here == 'G':\n",
    "        steps = 0\n",
    "        \n",
    "clear_output(wait=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
